{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8b8GVQrU66a9",
        "outputId": "7d374438-b835-45e7-89ab-b5073e4a5dcf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gradio in /usr/local/lib/python3.12/dist-packages (5.46.0)\n",
            "Collecting whisper-openai\n",
            "  Downloading whisper_openai-1.0.0-py3-none-any.whl.metadata (480 bytes)\n",
            "Collecting speechrecognition\n",
            "  Downloading speechrecognition-3.14.3-py3-none-any.whl.metadata (30 kB)\n",
            "Requirement already satisfied: pydub in /usr/local/lib/python3.12/dist-packages (0.25.1)\n",
            "Requirement already satisfied: librosa in /usr/local/lib/python3.12/dist-packages (0.11.0)\n",
            "Requirement already satisfied: plotly in /usr/local/lib/python3.12/dist-packages (5.24.1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (3.10.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (2.0.2)\n",
            "Collecting vosk\n",
            "  Downloading vosk-0.3.45-py3-none-manylinux_2_12_x86_64.manylinux2010_x86_64.whl.metadata (1.8 kB)\n",
            "Requirement already satisfied: aiofiles<25.0,>=22.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (24.1.0)\n",
            "Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (4.10.0)\n",
            "Requirement already satisfied: brotli>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (1.1.0)\n",
            "Requirement already satisfied: fastapi<1.0,>=0.115.2 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.116.2)\n",
            "Requirement already satisfied: ffmpy in /usr/local/lib/python3.12/dist-packages (from gradio) (0.6.1)\n",
            "Requirement already satisfied: gradio-client==1.13.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (1.13.0)\n",
            "Requirement already satisfied: groovy~=0.1 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.1.2)\n",
            "Requirement already satisfied: httpx<1.0,>=0.24.1 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.28.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.33.5 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.35.0)\n",
            "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (3.1.6)\n",
            "Requirement already satisfied: markupsafe<4.0,>=2.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (3.0.2)\n",
            "Requirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (3.11.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from gradio) (25.0)\n",
            "Requirement already satisfied: pillow<12.0,>=8.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (11.3.0)\n",
            "Requirement already satisfied: pydantic<2.12,>=2.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (2.11.9)\n",
            "Requirement already satisfied: python-multipart>=0.0.18 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.0.20)\n",
            "Requirement already satisfied: pyyaml<7.0,>=5.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (6.0.2)\n",
            "Requirement already satisfied: ruff>=0.9.3 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.13.0)\n",
            "Requirement already satisfied: safehttpx<0.2.0,>=0.1.6 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.1.6)\n",
            "Requirement already satisfied: semantic-version~=2.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (2.10.0)\n",
            "Requirement already satisfied: starlette<1.0,>=0.40.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.48.0)\n",
            "Requirement already satisfied: tomlkit<0.14.0,>=0.12.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.13.3)\n",
            "Requirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.17.4)\n",
            "Requirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (4.15.0)\n",
            "Requirement already satisfied: uvicorn>=0.14.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.35.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from gradio-client==1.13.0->gradio) (2025.3.0)\n",
            "Requirement already satisfied: websockets<16.0,>=10.0 in /usr/local/lib/python3.12/dist-packages (from gradio-client==1.13.0->gradio) (15.0.1)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (from whisper-openai) (2.8.0+cu126)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from whisper-openai) (4.67.1)\n",
            "Requirement already satisfied: more-itertools in /usr/local/lib/python3.12/dist-packages (from whisper-openai) (10.8.0)\n",
            "Requirement already satisfied: transformers>=4.19.0 in /usr/local/lib/python3.12/dist-packages (from whisper-openai) (4.56.1)\n",
            "Collecting ffmpeg-python==0.2.0 (from whisper-openai)\n",
            "  Downloading ffmpeg_python-0.2.0-py3-none-any.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.12/dist-packages (from ffmpeg-python==0.2.0->whisper-openai) (1.0.0)\n",
            "Requirement already satisfied: audioread>=2.1.9 in /usr/local/lib/python3.12/dist-packages (from librosa) (3.0.1)\n",
            "Requirement already satisfied: numba>=0.51.0 in /usr/local/lib/python3.12/dist-packages (from librosa) (0.60.0)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from librosa) (1.16.2)\n",
            "Requirement already satisfied: scikit-learn>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from librosa) (1.6.1)\n",
            "Requirement already satisfied: joblib>=1.0 in /usr/local/lib/python3.12/dist-packages (from librosa) (1.5.2)\n",
            "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.12/dist-packages (from librosa) (4.4.2)\n",
            "Requirement already satisfied: soundfile>=0.12.1 in /usr/local/lib/python3.12/dist-packages (from librosa) (0.13.1)\n",
            "Requirement already satisfied: pooch>=1.1 in /usr/local/lib/python3.12/dist-packages (from librosa) (1.8.2)\n",
            "Requirement already satisfied: soxr>=0.3.2 in /usr/local/lib/python3.12/dist-packages (from librosa) (1.0.0)\n",
            "Requirement already satisfied: lazy_loader>=0.1 in /usr/local/lib/python3.12/dist-packages (from librosa) (0.4)\n",
            "Requirement already satisfied: msgpack>=1.0 in /usr/local/lib/python3.12/dist-packages (from librosa) (1.1.1)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.12/dist-packages (from plotly) (8.5.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (4.60.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.4.9)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (3.2.4)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.12/dist-packages (from vosk) (2.0.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from vosk) (2.32.4)\n",
            "Collecting srt (from vosk)\n",
            "  Downloading srt-3.5.3.tar.gz (28 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.12/dist-packages (from anyio<5.0,>=3.0->gradio) (3.10)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.12/dist-packages (from anyio<5.0,>=3.0->gradio) (1.3.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.12/dist-packages (from cffi>=1.0->vosk) (2.23)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1.0,>=0.24.1->gradio) (2025.8.3)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1.0,>=0.24.1->gradio) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1.0,>=0.24.1->gradio) (0.16.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.33.5->gradio) (3.19.1)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.33.5->gradio) (1.1.10)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.12/dist-packages (from numba>=0.51.0->librosa) (0.43.0)\n",
            "Requirement already satisfied: platformdirs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from pooch>=1.1->librosa) (4.4.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<2.12,>=2.0->gradio) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<2.12,>=2.0->gradio) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<2.12,>=2.0->gradio) (0.4.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->vosk) (3.4.3)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->vosk) (2.5.0)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn>=1.1.0->librosa) (3.6.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers>=4.19.0->whisper-openai) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers>=4.19.0->whisper-openai) (0.22.0)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers>=4.19.0->whisper-openai) (0.6.2)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.12/dist-packages (from typer<1.0,>=0.12->gradio) (8.2.1)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from typer<1.0,>=0.12->gradio) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.12/dist-packages (from typer<1.0,>=0.12->gradio) (13.9.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch->whisper-openai) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch->whisper-openai) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch->whisper-openai) (3.5)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->whisper-openai) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->whisper-openai) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch->whisper-openai) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch->whisper-openai) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch->whisper-openai) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch->whisper-openai) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch->whisper-openai) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch->whisper-openai) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch->whisper-openai) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch->whisper-openai) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch->whisper-openai) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->whisper-openai) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch->whisper-openai) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch->whisper-openai) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch->whisper-openai) (3.4.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.19.2)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch->whisper-openai) (1.3.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.2)\n",
            "Downloading whisper_openai-1.0.0-py3-none-any.whl (1.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m13.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ffmpeg_python-0.2.0-py3-none-any.whl (25 kB)\n",
            "Downloading speechrecognition-3.14.3-py3-none-any.whl (32.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m32.9/32.9 MB\u001b[0m \u001b[31m26.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading vosk-0.3.45-py3-none-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (7.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.2/7.2 MB\u001b[0m \u001b[31m52.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: srt\n",
            "  Building wheel for srt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for srt: filename=srt-3.5.3-py3-none-any.whl size=22427 sha256=bbb2cc6bbe49feb1d341d4ea1293138ce0668958d40b75a988e87c8ad5d1706f\n",
            "  Stored in directory: /root/.cache/pip/wheels/7e/75/5b/e1d5c3756631e4bda806f6cc9640153b39484bb6f7b0b8def3\n",
            "Successfully built srt\n",
            "Installing collected packages: srt, speechrecognition, ffmpeg-python, vosk, whisper-openai\n",
            "Successfully installed ffmpeg-python-0.2.0 speechrecognition-3.14.3 srt-3.5.3 vosk-0.3.45 whisper-openai-1.0.0\n",
            "--2025-10-01 08:38:37--  https://alphacephei.com/vosk/models/vosk-model-small-en-us-0.15.zip\n",
            "Resolving alphacephei.com (alphacephei.com)... 188.40.21.16, 2a01:4f8:13a:279f::2\n",
            "Connecting to alphacephei.com (alphacephei.com)|188.40.21.16|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 41205931 (39M) [application/zip]\n",
            "Saving to: ‘vosk-model-small-en-us-0.15.zip’\n",
            "\n",
            "vosk-model-small-en 100%[===================>]  39.30M  17.4MB/s    in 2.3s    \n",
            "\n",
            "2025-10-01 08:38:40 (17.4 MB/s) - ‘vosk-model-small-en-us-0.15.zip’ saved [41205931/41205931]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Install required packages\n",
        "!pip install gradio whisper-openai speechrecognition pydub librosa plotly matplotlib pandas numpy vosk\n",
        "\n",
        "# Download Vosk model\n",
        "!wget https://alphacephei.com/vosk/models/vosk-model-small-en-us-0.15.zip\n",
        "!unzip -q vosk-model-small-en-us-0.15.zip\n",
        "\n",
        "import gradio as gr\n",
        "import whisper\n",
        "import speech_recognition as sr\n",
        "import librosa\n",
        "import librosa.display\n",
        "import matplotlib.pyplot as plt\n",
        "import plotly.express as px\n",
        "import plotly.graph_objects as go\n",
        "from plotly.subplots import make_subplots\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import io\n",
        "import tempfile\n",
        "import os\n",
        "from datetime import datetime\n",
        "from pydub import AudioSegment\n",
        "import wave\n",
        "import json\n",
        "from vosk import Model, KaldiRecognizer\n",
        "from typing import Dict, List, Tuple\n",
        "import time\n",
        "import base64"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class STTEngine:\n",
        "    def __init__(self):\n",
        "        self.recognizer = sr.Recognizer()\n",
        "        self.results_history = []\n",
        "        self.performance_metrics = []\n",
        "\n",
        "    def process_audio(self, audio_path: str, method: str) -> Dict:\n",
        "        \"\"\"Process audio with specified method\"\"\"\n",
        "        try:\n",
        "            if method == \"Whisper\":\n",
        "                return self._recognize_whisper(audio_path)\n",
        "            elif method == \"Vosk\":\n",
        "                return self._recognize_vosk(audio_path)\n",
        "            elif method == \"Google API\":\n",
        "                return self._recognize_google(audio_path)\n",
        "            else:\n",
        "                return {\n",
        "                    \"text\": \"Invalid method selected\",\n",
        "                    \"success\": False,\n",
        "                    \"processing_time\": 0,\n",
        "                    \"method\": method\n",
        "                }\n",
        "        except Exception as e:\n",
        "            return {\n",
        "                \"text\": f\"Error in {method}: {str(e)}\",\n",
        "                \"success\": False,\n",
        "                \"processing_time\": 0,\n",
        "                \"method\": method\n",
        "            }\n",
        "\n",
        "    def _recognize_whisper(self, audio_path: str) -> Dict:\n",
        "        \"\"\"Whisper recognition\"\"\"\n",
        "        start_time = time.time()\n",
        "        try:\n",
        "            model = whisper.load_model(\"base\")\n",
        "            result = model.transcribe(audio_path)\n",
        "            processing_time = time.time() - start_time\n",
        "            return {\n",
        "                \"text\": result[\"text\"].strip(),\n",
        "                \"success\": True,\n",
        "                \"processing_time\": processing_time,\n",
        "                \"method\": \"Whisper\"\n",
        "            }\n",
        "        except Exception as e:\n",
        "            return {\n",
        "                \"text\": f\"Whisper Error: {str(e)}\",\n",
        "                \"success\": False,\n",
        "                \"processing_time\": time.time() - start_time,\n",
        "                \"method\": \"Whisper\"\n",
        "            }\n",
        "\n",
        "    def _recognize_vosk(self, audio_path: str) -> Dict:\n",
        "        \"\"\"Vosk recognition\"\"\"\n",
        "        start_time = time.time()\n",
        "        try:\n",
        "            model = Model(\"vosk-model-small-en-us-0.15\")\n",
        "\n",
        "            # Ensure audio is in correct format\n",
        "            if not audio_path.endswith('.wav'):\n",
        "                audio_path = self._convert_to_wav(audio_path)\n",
        "\n",
        "            wf = wave.open(audio_path, \"rb\")\n",
        "\n",
        "            # Check audio format\n",
        "            if wf.getnchannels() != 1 or wf.getsampwidth() != 2 or wf.getcomptype() != \"NONE\":\n",
        "                converted_path = self._convert_audio_format(audio_path)\n",
        "                wf.close()\n",
        "                wf = wave.open(converted_path, \"rb\")\n",
        "\n",
        "            rec = KaldiRecognizer(model, wf.getframerate())\n",
        "            rec.SetWords(True)\n",
        "\n",
        "            results = []\n",
        "            while True:\n",
        "                data = wf.readframes(4000)\n",
        "                if len(data) == 0:\n",
        "                    break\n",
        "                if rec.AcceptWaveform(data):\n",
        "                    part_result = json.loads(rec.Result())\n",
        "                    results.append(part_result.get('text', ''))\n",
        "\n",
        "            final_result = json.loads(rec.FinalResult())\n",
        "            text = ' '.join(results + [final_result.get('text', '')]).strip()\n",
        "\n",
        "            wf.close()\n",
        "            return {\n",
        "                \"text\": text if text else \"No speech detected\",\n",
        "                \"success\": bool(text and text != \"No speech detected\"),\n",
        "                \"processing_time\": time.time() - start_time,\n",
        "                \"method\": \"Vosk\"\n",
        "            }\n",
        "\n",
        "        except Exception as e:\n",
        "            return {\n",
        "                \"text\": f\"Vosk Error: {str(e)}\",\n",
        "                \"success\": False,\n",
        "                \"processing_time\": time.time() - start_time,\n",
        "                \"method\": \"Vosk\"\n",
        "            }\n",
        "\n",
        "    def _recognize_google(self, audio_path: str) -> Dict:\n",
        "        \"\"\"Google Speech API recognition\"\"\"\n",
        "        start_time = time.time()\n",
        "        try:\n",
        "            # Convert to WAV if needed\n",
        "            if not audio_path.endswith('.wav'):\n",
        "                audio_path = self._convert_to_wav(audio_path)\n",
        "\n",
        "            with sr.AudioFile(audio_path) as source:\n",
        "                audio_data = self.recognizer.record(source)\n",
        "                text = self.recognizer.recognize_google(audio_data)\n",
        "                return {\n",
        "                    \"text\": text,\n",
        "                    \"success\": True,\n",
        "                    \"processing_time\": time.time() - start_time,\n",
        "                    \"method\": \"Google API\"\n",
        "                }\n",
        "        except sr.UnknownValueError:\n",
        "            return {\n",
        "                \"text\": \"Could not understand audio\",\n",
        "                \"success\": False,\n",
        "                \"processing_time\": time.time() - start_time,\n",
        "                \"method\": \"Google API\"\n",
        "            }\n",
        "        except sr.RequestError as e:\n",
        "            return {\n",
        "                \"text\": f\"Google API Error: {e}\",\n",
        "                \"success\": False,\n",
        "                \"processing_time\": time.time() - start_time,\n",
        "                \"method\": \"Google API\"\n",
        "            }\n",
        "        except Exception as e:\n",
        "            return {\n",
        "                \"text\": f\"Google API Exception: {str(e)}\",\n",
        "                \"success\": False,\n",
        "                \"processing_time\": time.time() - start_time,\n",
        "                \"method\": \"Google API\"\n",
        "            }\n",
        "\n",
        "    def _convert_to_wav(self, audio_path: str) -> str:\n",
        "        \"\"\"Convert audio file to WAV format\"\"\"\n",
        "        try:\n",
        "            if audio_path.endswith('.wav'):\n",
        "                return audio_path\n",
        "\n",
        "            output_path = audio_path + '.wav'\n",
        "            audio = AudioSegment.from_file(audio_path)\n",
        "            audio.export(output_path, format=\"wav\")\n",
        "            return output_path\n",
        "        except Exception as e:\n",
        "            print(f\"Conversion error: {e}\")\n",
        "            return audio_path\n",
        "\n",
        "    def _convert_audio_format(self, audio_path: str) -> str:\n",
        "        \"\"\"Convert audio to proper format for Vosk\"\"\"\n",
        "        try:\n",
        "            output_path = \"converted_audio.wav\"\n",
        "            audio = AudioSegment.from_file(audio_path)\n",
        "            audio = audio.set_channels(1)  # Mono\n",
        "            audio = audio.set_frame_rate(16000)  # 16kHz\n",
        "            audio = audio.set_sample_width(2)  # 16-bit\n",
        "            audio.export(output_path, format=\"wav\")\n",
        "            return output_path\n",
        "        except Exception as e:\n",
        "            print(f\"Audio conversion error: {e}\")\n",
        "            return audio_path\n",
        "\n",
        "    def compare_all_methods(self, audio_path: str, audio_type: str = \"Custom\") -> Dict:\n",
        "        \"\"\"Compare all three methods with proper error handling\"\"\"\n",
        "        methods = [\"Whisper\", \"Vosk\", \"Google API\"]\n",
        "        results = {}\n",
        "\n",
        "        for method in methods:\n",
        "            print(f\"Processing with {method}...\")\n",
        "            result = self.process_audio(audio_path, method)\n",
        "            # Use consistent keys (lowercase with underscore)\n",
        "            key = method.lower().replace(' ', '_')\n",
        "            results[key] = result\n",
        "            self.performance_metrics.append(result)\n",
        "\n",
        "        comparison_result = {\n",
        "            'audio_type': audio_type,\n",
        "            'timestamp': datetime.now(),\n",
        "            **results\n",
        "        }\n",
        "        self.results_history.append(comparison_result)\n",
        "\n",
        "        print(f\"Comparison completed. Results keys: {list(comparison_result.keys())}\")\n",
        "        return comparison_result\n",
        "\n",
        "# Initialize engine\n",
        "stt_engine = STTEngine()"
      ],
      "metadata": {
        "id": "pJjpq24a67wT"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_performance_dashboard() -> go.Figure:\n",
        "    \"\"\"Create performance dashboard from history - FIXED VERSION\"\"\"\n",
        "    if not stt_engine.performance_metrics:\n",
        "        # Return empty figure if no data\n",
        "        fig = go.Figure()\n",
        "        fig.add_annotation(\n",
        "            text=\"📊 No performance data available yet<br>Process some audio files to see analytics!\",\n",
        "            xref=\"paper\", yref=\"paper\",\n",
        "            x=0.5, y=0.5,\n",
        "            showarrow=False,\n",
        "            font=dict(size=16, color=\"gray\"),\n",
        "            align=\"center\"\n",
        "        )\n",
        "        fig.update_layout(\n",
        "            title_text=\"Performance Analytics Dashboard\",\n",
        "            height=400\n",
        "        )\n",
        "        return fig\n",
        "\n",
        "    # Create DataFrame from performance metrics\n",
        "    df = pd.DataFrame(stt_engine.performance_metrics)\n",
        "\n",
        "    # Debug: Print what we have\n",
        "    print(f\"Performance metrics columns: {df.columns.tolist()}\")\n",
        "    print(f\"Methods found: {df['method'].unique()}\")\n",
        "    print(f\"Total records: {len(df)}\")\n",
        "\n",
        "    # Calculate success rates\n",
        "    success_rates = df.groupby('method')['success'].mean() * 100\n",
        "\n",
        "    # Calculate average processing times\n",
        "    avg_times = df.groupby('method')['processing_time'].mean()\n",
        "\n",
        "    # Create dashboard with 2x2 layout\n",
        "    fig = make_subplots(\n",
        "        rows=2, cols=2,\n",
        "        subplot_titles=(\n",
        "            '🎯 Success Rates by Method',\n",
        "            '⏱️ Average Processing Time',\n",
        "            '📊 Processing Time Distribution',\n",
        "            '📈 Performance Overview'\n",
        "        ),\n",
        "        specs=[\n",
        "            [{\"type\": \"bar\"}, {\"type\": \"bar\"}],\n",
        "            [{\"type\": \"box\"}, {\"type\": \"scatter\"}]\n",
        "        ],\n",
        "        vertical_spacing=0.12,\n",
        "        horizontal_spacing=0.1\n",
        "    )\n",
        "\n",
        "    # 1. Success rates (Bar chart)\n",
        "    if not success_rates.empty:\n",
        "        fig.add_trace(\n",
        "            go.Bar(\n",
        "                x=success_rates.index,\n",
        "                y=success_rates.values,\n",
        "                name=\"Success Rate\",\n",
        "                marker_color=['#28a745' if rate > 50 else '#dc3545' for rate in success_rates.values],\n",
        "                text=[f\"{rate:.1f}%\" for rate in success_rates.values],\n",
        "                textposition='auto'\n",
        "            ),\n",
        "            row=1, col=1\n",
        "        )\n",
        "\n",
        "    # 2. Average processing times (Bar chart)\n",
        "    if not avg_times.empty:\n",
        "        fig.add_trace(\n",
        "            go.Bar(\n",
        "                x=avg_times.index,\n",
        "                y=avg_times.values,\n",
        "                name=\"Avg Processing Time\",\n",
        "                marker_color='#007bff',\n",
        "                text=[f\"{time:.2f}s\" for time in avg_times.values],\n",
        "                textposition='auto'\n",
        "            ),\n",
        "            row=1, col=2\n",
        "        )\n",
        "\n",
        "    # 3. Processing time distribution (Box plot)\n",
        "    if not df.empty:\n",
        "        for method in df['method'].unique():\n",
        "            method_data = df[df['method'] == method]['processing_time']\n",
        "            if len(method_data) > 0:\n",
        "                fig.add_trace(\n",
        "                    go.Box(\n",
        "                        y=method_data,\n",
        "                        name=method,\n",
        "                        marker_color='#ffc107',\n",
        "                        boxpoints='all',\n",
        "                        jitter=0.3,\n",
        "                        pointpos=-1.8\n",
        "                    ),\n",
        "                    row=2, col=1\n",
        "                )\n",
        "\n",
        "    # 4. Performance scatter plot (Success vs Processing Time)\n",
        "    if not df.empty:\n",
        "        color_map = {'Whisper': '#28a745', 'Vosk': '#007bff', 'Google API': '#dc3545'}\n",
        "\n",
        "        for method in df['method'].unique():\n",
        "            method_data = df[df['method'] == method]\n",
        "            if len(method_data) > 0:\n",
        "                fig.add_trace(\n",
        "                    go.Scatter(\n",
        "                        x=method_data['processing_time'],\n",
        "                        y=[1 if s else 0 for s in method_data['success']],\n",
        "                        mode='markers',\n",
        "                        name=method,\n",
        "                        marker=dict(\n",
        "                            size=10,\n",
        "                            color=color_map.get(method, '#6c757d'),\n",
        "                            opacity=0.7,\n",
        "                            line=dict(width=1, color='DarkSlateGrey')\n",
        "                        ),\n",
        "                        text=[f\"Method: {method}<br>Success: {s}<br>Time: {t:.2f}s\"\n",
        "                              for s, t in zip(method_data['success'], method_data['processing_time'])],\n",
        "                        hovertemplate='<b>%{text}</b><extra></extra>'\n",
        "                    ),\n",
        "                    row=2, col=2\n",
        "                )\n",
        "\n",
        "    # Update layout\n",
        "    fig.update_layout(\n",
        "        height=700,\n",
        "        title_text=\"📊 Performance Analytics Dashboard\",\n",
        "        title_x=0.5,\n",
        "        showlegend=True,\n",
        "        template=\"plotly_white\",\n",
        "        font=dict(size=12)\n",
        "    )\n",
        "\n",
        "    # Update axes labels\n",
        "    fig.update_yaxes(title_text=\"Success Rate (%)\", row=1, col=1, range=[0, 100])\n",
        "    fig.update_yaxes(title_text=\"Time (seconds)\", row=1, col=2)\n",
        "    fig.update_yaxes(title_text=\"Processing Time (s)\", row=2, col=1)\n",
        "    fig.update_yaxes(title_text=\"Success (1=Yes, 0=No)\", row=2, col=2, tickvals=[0, 1])\n",
        "    fig.update_xaxes(title_text=\"Method\", row=1, col=1)\n",
        "    fig.update_xaxes(title_text=\"Method\", row=1, col=2)\n",
        "    fig.update_xaxes(title_text=\"Method\", row=2, col=1)\n",
        "    fig.update_xaxes(title_text=\"Processing Time (s)\", row=2, col=2)\n",
        "\n",
        "    return fig\n",
        "\n",
        "def create_success_timeline() -> go.Figure:\n",
        "    \"\"\"Create a timeline of success/failure over time\"\"\"\n",
        "    if not stt_engine.performance_metrics:\n",
        "        fig = go.Figure()\n",
        "        fig.add_annotation(text=\"No timeline data available\", x=0.5, y=0.5, showarrow=False)\n",
        "        return fig\n",
        "\n",
        "    df = pd.DataFrame(stt_engine.performance_metrics)\n",
        "    df['timestamp'] = pd.to_datetime(df.get('timestamp', pd.Timestamp.now()))\n",
        "    df = df.sort_values('timestamp')\n",
        "\n",
        "    fig = go.Figure()\n",
        "\n",
        "    color_map = {'Whisper': '#28a745', 'Vosk': '#007bff', 'Google API': '#dc3545'}\n",
        "\n",
        "    for method in df['method'].unique():\n",
        "        method_data = df[df['method'] == method]\n",
        "\n",
        "        # Success points\n",
        "        success_data = method_data[method_data['success'] == True]\n",
        "        if len(success_data) > 0:\n",
        "            fig.add_trace(go.Scatter(\n",
        "                x=success_data['timestamp'],\n",
        "                y=[method] * len(success_data),\n",
        "                mode='markers',\n",
        "                name=f'{method} - Success',\n",
        "                marker=dict(color=color_map.get(method, '#28a745'), size=12, symbol='circle'),\n",
        "                text=[f\"Time: {t:.2f}s\" for t in success_data['processing_time']]\n",
        "            ))\n",
        "\n",
        "        # Failure points\n",
        "        failure_data = method_data[method_data['success'] == False]\n",
        "        if len(failure_data) > 0:\n",
        "            fig.add_trace(go.Scatter(\n",
        "                x=failure_data['timestamp'],\n",
        "                y=[method] * len(failure_data),\n",
        "                mode='markers',\n",
        "                name=f'{method} - Failed',\n",
        "                marker=dict(color='#dc3545', size=12, symbol='x'),\n",
        "                text=[f\"Error: {t}\" for t in failure_data['text']]\n",
        "            ))\n",
        "\n",
        "    fig.update_layout(\n",
        "        title=\"📈 Recognition Success Timeline\",\n",
        "        xaxis_title=\"Time\",\n",
        "        yaxis_title=\"Method\",\n",
        "        height=400,\n",
        "        showlegend=True\n",
        "    )\n",
        "\n",
        "    return fig"
      ],
      "metadata": {
        "id": "_Lzq0oX97mfi"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def show_analytics():\n",
        "    \"\"\"Show performance analytics - ENHANCED VERSION\"\"\"\n",
        "    dashboard = create_performance_dashboard()\n",
        "    timeline = create_success_timeline()\n",
        "\n",
        "    if not stt_engine.performance_metrics:\n",
        "        analytics_text = \"\"\"\n",
        "## 📈 Performance Analytics\n",
        "\n",
        "**No performance data available yet.**\n",
        "\n",
        "### How to get started:\n",
        "1. Go to **\"Single Method Test\"** tab and process an audio file\n",
        "2. Or go to **\"Method Comparison\"** tab to compare all methods\n",
        "3. Return here to see your analytics!\n",
        "\n",
        "### Expected Metrics:\n",
        "- ✅ Success rates for each method\n",
        "- ⏱️ Processing time comparisons\n",
        "- 📊 Performance trends over time\n",
        "- 📈 Method reliability analysis\n",
        "\"\"\"\n",
        "    else:\n",
        "        df = pd.DataFrame(stt_engine.performance_metrics)\n",
        "        total_tests = len(df)\n",
        "        success_rate = df['success'].mean() * 100\n",
        "\n",
        "        # Method-specific statistics\n",
        "        method_stats = []\n",
        "        for method in df['method'].unique():\n",
        "            method_data = df[df['method'] == method]\n",
        "            method_success = method_data['success'].mean() * 100\n",
        "            avg_time = method_data['processing_time'].mean()\n",
        "            tests_count = len(method_data)\n",
        "            method_stats.append({\n",
        "                'method': method,\n",
        "                'success_rate': method_success,\n",
        "                'avg_time': avg_time,\n",
        "                'tests_count': tests_count\n",
        "            })\n",
        "\n",
        "        # Sort by success rate (descending)\n",
        "        method_stats.sort(key=lambda x: x['success_rate'], reverse=True)\n",
        "\n",
        "        analytics_text = f\"\"\"\n",
        "## 📈 Performance Analytics\n",
        "\n",
        "### 📊 Overall Statistics:\n",
        "- **Total Tests Conducted:** {total_tests}\n",
        "- **Overall Success Rate:** {success_rate:.1f}%\n",
        "- **Methods Compared:** {len(df['method'].unique())}\n",
        "\n",
        "### 🏆 Method Performance Ranking:\n",
        "\"\"\"\n",
        "\n",
        "        for i, stats in enumerate(method_stats, 1):\n",
        "            medal = \"🥇\" if i == 1 else \"🥈\" if i == 2 else \"🥉\" if i == 3 else \"📊\"\n",
        "            analytics_text += f\"\"\"\n",
        "{medal} **{stats['method']}:**\n",
        "   - Success Rate: {stats['success_rate']:.1f}%\n",
        "   - Average Time: {stats['avg_time']:.2f}s\n",
        "   - Tests: {stats['tests_count']}\n",
        "\"\"\"\n",
        "\n",
        "        # Additional insights\n",
        "        best_method = method_stats[0]['method'] if method_stats else \"N/A\"\n",
        "        fastest_method = min(method_stats, key=lambda x: x['avg_time'])['method'] if method_stats else \"N/A\"\n",
        "\n",
        "        analytics_text += f\"\"\"\n",
        "### 💡 Key Insights:\n",
        "- **Most Accurate:** {best_method}\n",
        "- **Fastest:** {fastest_method}\n",
        "- **Total Processing Time:** {df['processing_time'].sum():.2f}s\n",
        "\n",
        "### 📈 Recommendations:\n",
        "\"\"\"\n",
        "\n",
        "        if success_rate < 50:\n",
        "            analytics_text += \"- 🔴 **Low overall accuracy** - Try using clearer audio or different methods\"\n",
        "        elif success_rate < 80:\n",
        "            analytics_text += \"- 🟡 **Moderate accuracy** - Consider using the best performing method consistently\"\n",
        "        else:\n",
        "            analytics_text += \"- 🟢 **High accuracy** - System is performing well!\"\n",
        "\n",
        "        if any(stats['success_rate'] > 90 for stats in method_stats):\n",
        "            analytics_text += f\"\\n- ⭐ **{best_method}** is highly reliable (over 90% success rate)\"\n",
        "\n",
        "    return analytics_text, dashboard, timeline\n",
        "\n",
        "def clear_history():\n",
        "    \"\"\"Clear all history and metrics\"\"\"\n",
        "    stt_engine.results_history.clear()\n",
        "    stt_engine.performance_metrics.clear()\n",
        "\n",
        "    # Create empty dashboard\n",
        "    empty_dashboard = create_performance_dashboard()\n",
        "    empty_timeline = create_success_timeline()\n",
        "\n",
        "    analytics_text = \"\"\"\n",
        "## 📈 Performance Analytics\n",
        "\n",
        "**🗑️ History cleared!**\n",
        "\n",
        "### Next Steps:\n",
        "1. Process new audio files in the other tabs\n",
        "2. Return here to see fresh analytics\n",
        "3. Build up new performance data\n",
        "\n",
        "### What you'll see:\n",
        "- Success rates for each method\n",
        "- Processing time comparisons\n",
        "- Performance trends\n",
        "- Method recommendations\n",
        "\"\"\"\n",
        "\n",
        "    return analytics_text, empty_dashboard, empty_timeline"
      ],
      "metadata": {
        "id": "bnA-s7lv7pGt"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define CSS for better styling\n",
        "css = \"\"\"\n",
        ".gradio-container {\n",
        "    font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;\n",
        "}\n",
        ".header {\n",
        "    text-align: center;\n",
        "    padding: 20px;\n",
        "    background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);\n",
        "    color: white;\n",
        "    border-radius: 10px;\n",
        "    margin-bottom: 20px;\n",
        "}\n",
        ".success { color: #28a745; font-weight: bold; }\n",
        ".error { color: #dc3545; font-weight: bold; }\n",
        ".processing { color: #ffc107; font-weight: bold; }\n",
        ".analytics-panel {\n",
        "    background: #f8f9fa;\n",
        "    padding: 15px;\n",
        "    border-radius: 10px;\n",
        "    border-left: 4px solid #007bff;\n",
        "}\n",
        "\"\"\"\n",
        "\n",
        "# Create the Gradio interface\n",
        "with gr.Blocks(css=css, theme=gr.themes.Soft()) as demo:\n",
        "\n",
        "    gr.HTML(\"\"\"\n",
        "    <div class=\"header\">\n",
        "        <h1>🎤 Speech-to-Text Accessibility System</h1>\n",
        "        <p>Convert spoken commands to text using multiple recognition methods</p>\n",
        "        <p><strong>Perfect for accessibility applications and device control</strong></p>\n",
        "    </div>\n",
        "    \"\"\")\n",
        "\n",
        "    with gr.Tabs():\n",
        "        with gr.TabItem(\"🔍 Single Method Test\"):\n",
        "            with gr.Row():\n",
        "                with gr.Column():\n",
        "                    gr.Markdown(\"### 🎵 Upload or Record Audio\")\n",
        "                    audio_input_single = gr.Audio(\n",
        "                        label=\"Choose Audio File or Record Voice\",\n",
        "                        type=\"numpy\",\n",
        "                        sources=[\"upload\", \"microphone\"],\n",
        "                        waveform_options={\"show_controls\": True}\n",
        "                    )\n",
        "                    method_selector = gr.Radio(\n",
        "                        choices=[\"Whisper\", \"Vosk\", \"Google API\"],\n",
        "                        label=\"Recognition Method\",\n",
        "                        value=\"Whisper\",\n",
        "                        info=\"Choose which method to use for speech recognition\"\n",
        "                    )\n",
        "                    audio_type_single = gr.Textbox(\n",
        "                        label=\"Audio Type Description\",\n",
        "                        placeholder=\"e.g., Clear male voice, Noisy background...\",\n",
        "                        value=\"Custom\"\n",
        "                    )\n",
        "                    process_btn_single = gr.Button(\"🚀 Process Audio\", variant=\"primary\", size=\"lg\")\n",
        "\n",
        "                with gr.Column():\n",
        "                    gr.Markdown(\"### 📝 Recognition Results\")\n",
        "                    output_text_single = gr.Markdown(label=\"Results\", value=\"Results will appear here...\")\n",
        "                    status_indicator = gr.Textbox(\n",
        "                        label=\"Status\",\n",
        "                        value=\"Waiting for input...\",\n",
        "                        interactive=False\n",
        "                    )\n",
        "\n",
        "            with gr.Row():\n",
        "                with gr.Column():\n",
        "                    waveform_single = gr.Plot(label=\"📊 Waveform Visualization\")\n",
        "                with gr.Column():\n",
        "                    spectrogram_single = gr.Plot(label=\"🌈 Spectrogram Visualization\")\n",
        "\n",
        "        with gr.TabItem(\"📊 Method Comparison\"):\n",
        "            with gr.Row():\n",
        "                with gr.Column():\n",
        "                    gr.Markdown(\"### 🎵 Upload or Record Audio\")\n",
        "                    audio_input_compare = gr.Audio(\n",
        "                        label=\"Choose Audio File or Record Voice\",\n",
        "                        type=\"numpy\",\n",
        "                        sources=[\"upload\", \"microphone\"],\n",
        "                        waveform_options={\"show_controls\": True}\n",
        "                    )\n",
        "                    audio_type_compare = gr.Dropdown(\n",
        "                        choices=[\n",
        "                            \"Clear male voice\", \"Clear female voice\",\n",
        "                            \"Fast speech\", \"Noisy background\", \"Soft voice\", \"Custom\"\n",
        "                        ],\n",
        "                        label=\"Audio Type Category\",\n",
        "                        value=\"Custom\",\n",
        "                        info=\"Select the type of audio for testing\"\n",
        "                    )\n",
        "                    compare_btn = gr.Button(\"🔬 Compare All Methods\", variant=\"primary\", size=\"lg\")\n",
        "\n",
        "                with gr.Column():\n",
        "                    gr.Markdown(\"### 📊 Comparison Summary\")\n",
        "                    summary_output = gr.Markdown(label=\"Summary\", value=\"Comparison results will appear here...\")\n",
        "\n",
        "            gr.Markdown(\"### 📋 Detailed Results\")\n",
        "            results_table = gr.Dataframe(\n",
        "                label=\"Method Comparison Results\",\n",
        "                headers=[\"Method\", \"Result\", \"Status\", \"Processing Time\"],\n",
        "                interactive=False,\n",
        "                wrap=True\n",
        "            )\n",
        "\n",
        "            comparison_plot = gr.Plot(label=\"📈 Method Performance Comparison\")\n",
        "\n",
        "            with gr.Row():\n",
        "                with gr.Column():\n",
        "                    waveform_compare = gr.Plot(label=\"📊 Waveform\")\n",
        "                with gr.Column():\n",
        "                    spectrogram_compare = gr.Plot(label=\"🌈 Spectrogram\")\n",
        "\n",
        "        with gr.TabItem(\"📈 Analytics Dashboard\"):\n",
        "            with gr.Row():\n",
        "                with gr.Column():\n",
        "                    gr.Markdown(\"\"\"\n",
        "                    <div class=\"analytics-panel\">\n",
        "                    <h3>📊 Performance Analytics</h3>\n",
        "                    <p>View comprehensive analytics and performance metrics for all speech recognition methods.</p>\n",
        "                    </div>\n",
        "                    \"\"\")\n",
        "                    with gr.Row():\n",
        "                        analytics_btn = gr.Button(\"🔄 Refresh Analytics\", variant=\"primary\", size=\"lg\")\n",
        "                        clear_btn = gr.Button(\"🗑️ Clear History\", variant=\"secondary\", size=\"lg\")\n",
        "\n",
        "                    gr.Markdown(\"\"\"\n",
        "                    **📈 What you'll see:**\n",
        "                    - Success rates for each method\n",
        "                    - Processing time analysis\n",
        "                    - Performance trends\n",
        "                    - Method recommendations\n",
        "\n",
        "                    **💡 Tips:**\n",
        "                    - Process multiple audio files for better insights\n",
        "                    - Compare different audio types\n",
        "                    - Use the best performing method for your needs\n",
        "                    \"\"\")\n",
        "\n",
        "                with gr.Column():\n",
        "                    analytics_text = gr.Markdown(\n",
        "                        label=\"Analytics Summary\",\n",
        "                        value=\"## 📈 Performance Analytics\\n\\nProcess some audio files to see analytics here!\"\n",
        "                    )\n",
        "\n",
        "            gr.Markdown(\"### 📊 Performance Dashboard\")\n",
        "            analytics_dashboard = gr.Plot(label=\"Main Dashboard\")\n",
        "\n",
        "            gr.Markdown(\"### 📈 Success Timeline\")\n",
        "            analytics_timeline = gr.Plot(label=\"Timeline\")\n",
        "\n",
        "        with gr.TabItem(\"ℹ️ About & Help\"):\n",
        "            gr.Markdown(\"\"\"\n",
        "            ## 🎯 About This System\n",
        "\n",
        "            This Speech-to-Text Accessibility System provides multiple methods for converting spoken audio to text,\n",
        "            specifically designed for accessibility applications.\n",
        "\n",
        "            ### 🔧 Available Methods:\n",
        "\n",
        "            | Method | Type | Best For | Pros | Cons |\n",
        "            |--------|------|----------|------|------|\n",
        "            | **Whisper** | Offline | General purpose | High accuracy, multiple languages | Slower processing |\n",
        "            | **Vosk** | Offline | Real-time apps | Fast, lightweight | Lower accuracy |\n",
        "            | **Google API** | Online | High accuracy | Very accurate, robust | Requires internet |\n",
        "\n",
        "            ### 📊 Analytics Features:\n",
        "            - Real-time performance tracking\n",
        "            - Success rate comparisons\n",
        "            - Processing time analysis\n",
        "            - Method recommendations\n",
        "            - Historical data visualization\n",
        "            \"\"\")\n",
        "\n",
        "    # Event handlers\n",
        "    process_btn_single.click(\n",
        "        fn=process_single_method,\n",
        "        inputs=[audio_input_single, method_selector, audio_type_single],\n",
        "        outputs=[output_text_single, waveform_single, spectrogram_single, status_indicator]\n",
        "    )\n",
        "\n",
        "    compare_btn.click(\n",
        "        fn=process_comparison,\n",
        "        inputs=[audio_input_compare, audio_type_compare],\n",
        "        outputs=[summary_output, results_table, comparison_plot, waveform_compare, spectrogram_compare]\n",
        "    )\n",
        "\n",
        "    # Updated analytics handlers to include timeline\n",
        "    analytics_btn.click(\n",
        "        fn=show_analytics,\n",
        "        inputs=[],\n",
        "        outputs=[analytics_text, analytics_dashboard, analytics_timeline]\n",
        "    )\n",
        "\n",
        "    clear_btn.click(\n",
        "        fn=clear_history,\n",
        "        inputs=[],\n",
        "        outputs=[analytics_text, analytics_dashboard, analytics_timeline]\n",
        "    )\n",
        "\n",
        "    # Initial load\n",
        "    demo.load(\n",
        "        fn=show_analytics,\n",
        "        inputs=[],\n",
        "        outputs=[analytics_text, analytics_dashboard, analytics_timeline]\n",
        "    )\n",
        "\n",
        "# Launch the interface\n",
        "print(\"🚀 Launching Gradio Interface...\")\n",
        "print(\"📊 Performance dashboard is now fixed!\")\n",
        "print(\"⏳ This may take a few moments to load...\")\n",
        "\n",
        "try:\n",
        "    demo.launch(share=True, debug=False)\n",
        "except Exception as e:\n",
        "    print(f\"Note: Could not create public link. Launching locally...\")\n",
        "    demo.launch(debug=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 728
        },
        "id": "HpxR6hy57vHF",
        "outputId": "5416e684-a470-4960-c2a4-f2d5f3c63166"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/gradio/components/audio.py:200: UserWarning:\n",
            "\n",
            "The `show_controls` parameter is deprecated and will be removed in a future release. Use `show_recording_waveform` instead.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🚀 Launching Gradio Interface...\n",
            "📊 Performance dashboard is now fixed!\n",
            "⏳ This may take a few moments to load...\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://ad5969398b714d5a60.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://ad5969398b714d5a60.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}